2018-06-02 17:24:10,897 - lm_zh - INFO - Parse config file ...
2018-06-02 17:24:10,897 - lm_zh - INFO - Running with config: {'batch_size': 20, 'epoch': 13, 'init_scale': 0.1, 'embed_size': 200, 'num_sampled': 0, 'lr': 1.0, 'lr_decay': 0.5, 'lr_keep_epoch': 4, 'keep_prob': 1.0, 'num_steps': 20, 'num_layers': 2, 'max_grad_norm': 5.0, 'algo': 'lstm', 'hidden_size': 200, 'filter_w': 5, 'filter_size': 64, 'block_size': 1}
2018-06-02 17:24:10,897 - lm_zh - INFO - Build vocab and dataset ...
2018-06-02 17:24:10,918 - lm_zh - INFO - Save vocab to file data/weicheng/weicheng.vocab
2018-06-02 17:24:10,918 - lm_zh - INFO - Vocab size: 6002
2018-06-02 17:24:10,978 - lm_zh - INFO - Train dataset has 112800 words
2018-06-02 17:24:10,983 - lm_zh - INFO - Valid dataset has 6400 words
2018-06-02 17:24:10,983 - lm_zh - INFO - Build graph ...
2018-06-02 17:24:12,878 - lm_zh - INFO - Start train model ...
2018-06-02 17:24:12,882 - lm_zh - INFO - Start epoch 1 ...
2018-06-02 17:24:16,208 - lm_zh - INFO - Epoch 1 perplexity: 809.579
2018-06-02 17:24:16,209 - lm_zh - INFO - Start eval on valid data ...
2018-06-02 17:24:16,992 - lm_zh - INFO - Valid data perplexity: 643.399
2018-06-02 17:24:16,992 - lm_zh - INFO - Save model to weicheng/lstm/, with prefix lstm ...
2018-06-02 17:24:17,055 - lm_zh - INFO - Start epoch 2 ...
2018-06-02 17:24:20,136 - lm_zh - INFO - Epoch 2 perplexity: 494.266
2018-06-02 17:24:20,136 - lm_zh - INFO - Start eval on valid data ...
2018-06-02 17:24:20,207 - lm_zh - INFO - Valid data perplexity: 462.556
2018-06-02 17:24:20,207 - lm_zh - INFO - Save model to weicheng/lstm/, with prefix lstm ...
2018-06-02 17:24:20,260 - lm_zh - INFO - Start epoch 3 ...
2018-06-02 17:24:23,358 - lm_zh - INFO - Epoch 3 perplexity: 384.417
2018-06-02 17:24:23,358 - lm_zh - INFO - Start eval on valid data ...
2018-06-02 17:24:23,428 - lm_zh - INFO - Valid data perplexity: 418.217
2018-06-02 17:24:23,428 - lm_zh - INFO - Save model to weicheng/lstm/, with prefix lstm ...
2018-06-02 17:24:23,479 - lm_zh - INFO - Start epoch 4 ...
2018-06-02 17:24:26,570 - lm_zh - INFO - Epoch 4 perplexity: 319.264
2018-06-02 17:24:26,570 - lm_zh - INFO - Start eval on valid data ...
2018-06-02 17:24:26,641 - lm_zh - INFO - Valid data perplexity: 386.605
2018-06-02 17:24:26,641 - lm_zh - INFO - Save model to weicheng/lstm/, with prefix lstm ...
2018-06-02 17:24:26,696 - lm_zh - INFO - Start epoch 5 ...
2018-06-02 17:24:29,762 - lm_zh - INFO - Epoch 5 perplexity: 238.527
2018-06-02 17:24:29,762 - lm_zh - INFO - Start eval on valid data ...
2018-06-02 17:24:29,832 - lm_zh - INFO - Valid data perplexity: 376.828
2018-06-02 17:24:29,832 - lm_zh - INFO - Save model to weicheng/lstm/, with prefix lstm ...
2018-06-02 17:24:29,884 - lm_zh - INFO - Start epoch 6 ...
2018-06-02 17:24:32,968 - lm_zh - INFO - Epoch 6 perplexity: 185.455
2018-06-02 17:24:32,968 - lm_zh - INFO - Start eval on valid data ...
2018-06-02 17:24:33,038 - lm_zh - INFO - Valid data perplexity: 387.272
2018-06-02 17:24:33,039 - lm_zh - INFO - Start epoch 7 ...
2018-06-02 17:24:36,117 - lm_zh - INFO - Epoch 7 perplexity: 154.011
2018-06-02 17:24:36,118 - lm_zh - INFO - Start eval on valid data ...
2018-06-02 17:24:36,191 - lm_zh - INFO - Valid data perplexity: 404.969
2018-06-02 17:24:36,191 - lm_zh - INFO - Start epoch 8 ...
2018-06-02 17:24:39,263 - lm_zh - INFO - Epoch 8 perplexity: 137.627
2018-06-02 17:24:39,263 - lm_zh - INFO - Start eval on valid data ...
2018-06-02 17:24:39,331 - lm_zh - INFO - Valid data perplexity: 420.719
2018-06-02 17:24:39,332 - lm_zh - INFO - Start epoch 9 ...
2018-06-02 17:24:42,404 - lm_zh - INFO - Epoch 9 perplexity: 129.179
2018-06-02 17:24:42,404 - lm_zh - INFO - Start eval on valid data ...
2018-06-02 17:24:42,477 - lm_zh - INFO - Valid data perplexity: 426.408
2018-06-02 17:24:42,477 - lm_zh - INFO - Start epoch 10 ...
2018-06-02 17:24:45,568 - lm_zh - INFO - Epoch 10 perplexity: 124.890
2018-06-02 17:24:45,568 - lm_zh - INFO - Start eval on valid data ...
2018-06-02 17:24:45,641 - lm_zh - INFO - Valid data perplexity: 431.317
2018-06-02 17:24:45,641 - lm_zh - INFO - Start epoch 11 ...
2018-06-02 17:24:48,701 - lm_zh - INFO - Epoch 11 perplexity: 122.713
2018-06-02 17:24:48,701 - lm_zh - INFO - Start eval on valid data ...
2018-06-02 17:24:48,772 - lm_zh - INFO - Valid data perplexity: 434.455
2018-06-02 17:24:48,772 - lm_zh - INFO - Start epoch 12 ...
2018-06-02 17:24:51,874 - lm_zh - INFO - Epoch 12 perplexity: 121.616
2018-06-02 17:24:51,874 - lm_zh - INFO - Start eval on valid data ...
2018-06-02 17:24:51,944 - lm_zh - INFO - Valid data perplexity: 435.468
2018-06-02 17:24:51,944 - lm_zh - INFO - Start epoch 13 ...
2018-06-02 17:24:55,014 - lm_zh - INFO - Epoch 13 perplexity: 121.062
2018-06-02 17:24:55,014 - lm_zh - INFO - Start eval on valid data ...
2018-06-02 17:24:55,082 - lm_zh - INFO - Valid data perplexity: 436.577
2018-06-02 17:24:55,082 - lm_zh - INFO - Train done
2018-06-02 17:25:02,095 - lm_zh - INFO - Parse config file ...
2018-06-02 17:25:02,095 - lm_zh - INFO - Running with config: {'batch_size': 20, 'epoch': 13, 'init_scale': 0.1, 'embed_size': 200, 'num_sampled': 0, 'lr': 1.0, 'lr_decay': 0.5, 'lr_keep_epoch': 4, 'keep_prob': 1.0, 'num_steps': 20, 'num_layers': 2, 'max_grad_norm': 5.0, 'algo': 'lstm', 'hidden_size': 200, 'filter_w': 5, 'filter_size': 64, 'block_size': 1}
2018-06-02 17:25:02,095 - lm_zh - INFO - Build vocab and dataset ...
2018-06-02 17:25:02,099 - lm_zh - INFO - Vocab size: 6002
2018-06-02 17:25:02,111 - lm_zh - INFO - Test dataset has 8000 words
2018-06-02 17:25:02,111 - lm_zh - INFO - Build graph ...
2018-06-02 17:25:03,952 - lm_zh - INFO - Restore model ...
2018-06-02 17:25:03,953 - lm_zh - INFO - Model restore from weicheng/lstm/, with prefix lstm ...
2018-06-02 17:25:03,976 - lm_zh - INFO - Start test model ...
2018-06-02 17:25:03,978 - lm_zh - INFO - Start eval on test data ...
2018-06-02 17:25:04,189 - lm_zh - INFO - Test data perplexity: 312.213
2018-06-02 17:25:04,189 - lm_zh - INFO - Test done
